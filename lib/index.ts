import OpenAI from 'openai';
import { RequestOptions } from 'openai/core';
import {
  ChatCompletion,
  ChatCompletionChunk,
  ChatCompletionCreateParams,
  ChatCompletionMessageParam,
  Completion,
  CompletionCreateParams,
} from 'openai/resources';
import { Stream } from 'openai/streaming';
import { v4 as uuid } from 'uuid';
import { RequestParameters, resolvedQuery } from './utils/request';

/**
 * Represents the data structure for a chat completion.
 * Object keys represent a column name and the values represent the column value.
 */
export interface StreamingData {
  [columnName: string]: any;

  /**
   * The total estimated cost of the chat completion in USD. Optional.
   */
  cost?: number;

  /**
   * The latency of the chat completion in milliseconds. Optional.
   */
  latency?: number;

  /**
   * The output string generated by the chat completion.
   */
  output: string;

  /**
   * A timestamp representing when the chat completion occurred. Optional.
   */
  timestamp?: number;

  /**
   * The number of tokens used in the chat completion. Optional.
   */
  tokens?: number;
}

/**
 * Configuration settings for uploading chat completion data to Openlayer.
 */
interface StreamingDataConfig {
  /**
   * The name of the column that stores the request cost data. Can be null.
   */
  costColumnName: string | null;

  /**
   * The name of the column that stores the ground truth data. Can be null.
   */
  groundTruthColumnName: string | null;

  /**
   * The name of the column that stores inference IDs. Can be null.
   */
  inferenceIdColumnName: string | null;

  /**
   * An array of names for input variable columns. Can be null.
   */
  inputVariableNames?: string[] | null;

  /**
   * The name of the column that stores latency data. Can be null.
   */
  latencyColumnName: string | null;

  /**
   * The name of the column that stores the number of tokens. Can be null.
   */
  numOfTokenColumnName: string | null;

  /**
   * The name of the column that stores output data. Can be null.
   */
  outputColumnName: string | null;

  /**
   * The full prompt history for the chat completion.
   */
  prompt?: ChatCompletionMessageParam[];

  /**
   * The name of the column that stores timestamp data. Can be null.
   */
  timestampColumnName: string | null;
}

type OpenlayerClientConstructorProps = {
  openlayerApiKey?: string;
  openlayerInferencePipelineName?: string;
  openlayerProjectName?: string;
  openlayerServerUrl?: string;
};

type OpenAIMonitorConstructorProps = OpenlayerClientConstructorProps & {
  openAiApiKey: string;
  openlayerInferencePipelineName?: string;
  openlayerProjectName: string;
};

type OpenlayerInferencePipeline = {
  dataVolumeGraphs?: OpenlayerSampleVolumeGraph;
  dateCreated: string;
  dateLastEvaluated?: string;
  dateLastSampleReceived?: string;
  dateOfNextEvaluation?: string;
  dateUpdated: string;
  description?: string;
  failingGoalCount: number;
  id: string;
  name: string;
  passingGoalCount: number;
  projectId: string;
  status: OpenlayerInferencePipelineStatus;
  statusMessage?: string;
  totalGoalCount: number;
};

type OpenlayerInferencePipelineStatus =
  | 'completed'
  | 'failed'
  | 'paused'
  | 'queued'
  | 'running'
  | 'unknown';

type OpenlayerProject = {
  dateCreated: string;
  dateUpdated: string;
  description?: string;
  developmentGoalCount: number;
  goalCount: number;
  id: string;
  inferencePipelineCount: number;
  memberIds: string[];
  monitoringGoalCount: number;
  name: string;
  sample?: boolean;
  slackChannelId?: string;
  slackChannelName?: string;
  slackChannelNotificationsEnabled: boolean;
  taskType: OpenlayerTaskType;
  unreadNotificationCount: number;
  versionCount: number;
};

type OpenlayerSampleVolumeGraphBucket = {
  title: string;
  xAxis: {
    data: string[];
    title: string;
  };
  yAxis: {
    data: number[];
    title: string;
  };
};

type OpenlayerSampleVolumeGraph = {
  daily: OpenlayerSampleVolumeGraphBucket;
  hourly: OpenlayerSampleVolumeGraphBucket;
  monthly: OpenlayerSampleVolumeGraphBucket;
  weekly: OpenlayerSampleVolumeGraphBucket;
};

type OpenlayerTaskType =
  | 'llm-base'
  | 'tabular-classification'
  | 'tabular-regression'
  | 'text-classification';

type Pricing = {
  input: number;
  output: number;
};

const OpenAIPricing: { [key: string]: Pricing } = {
  'babbage-002': {
    input: 0.0004,
    output: 0.0004,
  },
  'davinci-002': {
    input: 0.002,
    output: 0.002,
  },
  'gpt-3.5-turbo': {
    input: 0.0005,
    output: 0.0015,
  },
  'gpt-3.5-turbo-0125': {
    input: 0.0005,
    output: 0.0015,
  },
  'gpt-3.5-turbo-0301': {
    input: 0.0015,
    output: 0.002,
  },
  'gpt-3.5-turbo-0613': {
    input: 0.0015,
    output: 0.002,
  },
  'gpt-3.5-turbo-1106': {
    input: 0.001,
    output: 0.002,
  },
  'gpt-3.5-turbo-16k-0613': {
    input: 0.003,
    output: 0.004,
  },
  'gpt-3.5-turbo-instruct': {
    input: 0.0015,
    output: 0.002,
  },
  'gpt-4': {
    input: 0.03,
    output: 0.06,
  },
  'gpt-4-0125-preview': {
    input: 0.01,
    output: 0.03,
  },
  'gpt-4-0314': {
    input: 0.03,
    output: 0.06,
  },
  'gpt-4-0613': {
    input: 0.03,
    output: 0.06,
  },
  'gpt-4-1106-preview': {
    input: 0.01,
    output: 0.03,
  },
  'gpt-4-1106-vision-preview': {
    input: 0.01,
    output: 0.03,
  },
  'gpt-4-32k': {
    input: 0.06,
    output: 0.12,
  },
  'gpt-4-32k-0314': {
    input: 0.06,
    output: 0.12,
  },
  'gpt-4-32k-0613': {
    input: 0.03,
    output: 0.06,
  },
};

export class OpenlayerClient {
  private openlayerApiKey?: string;

  public defaultConfig: StreamingDataConfig = {
    costColumnName: 'cost',
    groundTruthColumnName: null,
    inferenceIdColumnName: 'id',
    latencyColumnName: 'latency',
    numOfTokenColumnName: 'tokens',
    outputColumnName: 'output',
    timestampColumnName: 'timestamp',
  };

  private openlayerServerUrl: string = 'https://api.openlayer.com/v1';

  private version = '0.1.0a21';

  /**
   * Constructs an OpenlayerClient instance.
   * @param {OpenlayerClientConstructorProps} props - The config for the Openlayer client. The API key is required.
   * @throws {Error} Throws an error if the Openlayer API key is not provided.
   */
  constructor({
    openlayerApiKey,
    openlayerServerUrl,
  }: OpenlayerClientConstructorProps) {
    this.openlayerApiKey = openlayerApiKey;

    if (openlayerServerUrl) {
      this.openlayerServerUrl = openlayerServerUrl;
    }

    if (!this.openlayerApiKey) {
      console.error('Openlayer API key are required for publishing.');
    }
  }

  private resolvedQuery = (endpoint: string, args: RequestParameters = {}) =>
    resolvedQuery(this.openlayerServerUrl, endpoint, args);

  /**
   * Creates a new inference pipeline in Openlayer or loads an existing one.
   * @param {string} projectId - The ID of the project containing the inference pipeline.
   * @param {string} [name='production'] - The name of the inference pipeline, defaults to 'production'.
   * @returns {Promise<OpenlayerInferencePipeline>} A promise that resolves to an OpenlayerInferencePipeline object.
   * @throws {Error} Throws an error if the inference pipeline cannot be created or found.
   */
  public createInferencePipeline = async (
    projectId: string,
    name: string = 'production'
  ): Promise<OpenlayerInferencePipeline> => {
    try {
      return await this.loadInferencePipeline(projectId, name);
    } catch {}

    const createInferencePipelineEndpoint = `/projects/${projectId}/inference-pipelines`;
    const createInferencePipelineQuery = this.resolvedQuery(
      createInferencePipelineEndpoint,
      { version: this.version }
    );

    const createInferencePipelineResponse = await fetch(
      createInferencePipelineQuery,
      {
        body: JSON.stringify({
          description: '',
          name,
        }),
        headers: {
          Authorization: `Bearer ${this.openlayerApiKey}`,
          'Content-Type': 'application/json',
        },
        method: 'POST',
      }
    );

    const inferencePipeline = await createInferencePipelineResponse.json();

    if (!inferencePipeline?.id) {
      throw new Error('Error creating inference pipeline');
    }

    return inferencePipeline;
  };

  /**
   * Creates a new project in Openlayer or loads an existing one.
   * @param {string} name - The name of the project.
   * @param {OpenlayerTaskType} taskType - The type of task associated with the project.
   * @param {string} [description] - Optional description of the project.
   * @returns {Promise<OpenlayerProject>} A promise that resolves to an OpenlayerProject object.
   * @throws {Error} Throws an error if the project cannot be created or found.
   */
  public createProject = async (
    name: string,
    taskType: OpenlayerTaskType,
    description?: string
  ): Promise<OpenlayerProject> => {
    try {
      return await this.loadProject(name);
    } catch {}

    const projectsEndpoint = '/projects';
    const projectsQuery = this.resolvedQuery(projectsEndpoint);

    const response = await fetch(projectsQuery, {
      body: JSON.stringify({
        description,
        name,
        taskType,
      }),
      headers: {
        Authorization: `Bearer ${this.openlayerApiKey}`,
        'Content-Type': 'application/json',
      },
      method: 'POST',
    });

    const data = await response.json();
    const { items: projects, error } = data;

    if (!Array.isArray(projects)) {
      throw new Error(
        typeof error === 'string' ? error : 'Invalid response from Openlayer'
      );
    }

    const project = projects.find((p) => p.name === name);

    if (!project?.id) {
      throw new Error('Project not found');
    }

    return project;
  };

  /**
   * Loads an existing inference pipeline from Openlayer based on its name and project ID.
   * @param {string} projectId - The ID of the project containing the inference pipeline.
   * @param {string} [name='production'] - The name of the inference pipeline, defaults to 'production'.
   * @returns {Promise<OpenlayerInferencePipeline>} A promise that resolves to an OpenlayerInferencePipeline object.
   * @throws {Error} Throws an error if the inference pipeline is not found.
   */
  public loadInferencePipeline = async (
    projectId: string,
    name: string = 'production'
  ): Promise<OpenlayerInferencePipeline> => {
    const inferencePipelineEndpoint = `/projects/${projectId}/inference-pipelines`;
    const inferencePipelineQueryParameters = {
      name,
      version: this.version,
    };

    const inferencePipelineQuery = this.resolvedQuery(
      inferencePipelineEndpoint,
      inferencePipelineQueryParameters
    );

    const inferencePipelineResponse = await fetch(inferencePipelineQuery, {
      headers: {
        Authorization: `Bearer ${this.openlayerApiKey}`,
        'Content-Type': 'application/json',
      },
      method: 'GET',
    });

    const { items: inferencePipelines, error } =
      await inferencePipelineResponse.json();
    const inferencePipeline = Array.isArray(inferencePipelines)
      ? inferencePipelines.find((p) => p.name === name)
      : undefined;

    if (!inferencePipeline?.id) {
      throw new Error(
        typeof error === 'string' ? error : 'Inference pipeline not found'
      );
    }

    return inferencePipeline;
  };

  /**
   * Loads an existing project from Openlayer based on its name.
   * @param {string} name - The name of the project.
   * @returns {Promise<OpenlayerProject>} A promise that resolves to an OpenlayerProject object.
   * @throws {Error} Throws an error if the project is not found.
   */
  public loadProject = async (name: string): Promise<OpenlayerProject> => {
    const projectsEndpoint = '/projects';
    const projectsQueryParameters = {
      name,
      version: this.version,
    };

    const projectsQuery = this.resolvedQuery(
      projectsEndpoint,
      projectsQueryParameters
    );

    const response = await fetch(projectsQuery, {
      headers: {
        Authorization: `Bearer ${this.openlayerApiKey}`,
        'Content-Type': 'application/json',
      },
      method: 'GET',
    });

    const data = await response.json();
    const { items: projects, error } = data;

    if (!Array.isArray(projects)) {
      throw new Error(
        typeof error === 'string' ? error : 'Invalid response from Openlayer'
      );
    }

    const project = projects.find((p) => p.name === name);

    if (!project?.id) {
      throw new Error('Project not found');
    }

    return project;
  };

  /**
   * Streams data to the Openlayer inference pipeline.
   * @param {StreamingData} data - The chat completion data to be streamed.
   * @param {string} inferencePipelineId - The ID of the Openlayer inference pipeline to which data is streamed.
   * @returns {Promise<void>} A promise that resolves when the data has been successfully streamed.
   */
  public streamData = async (
    data: StreamingData,
    config: StreamingDataConfig,
    inferencePipelineId: string
  ): Promise<void> => {
    if (!this.openlayerApiKey) {
      console.error('Openlayer API key are required for streaming data.');
      return;
    }

    try {
      const dataStreamEndpoint = `/inference-pipelines/${inferencePipelineId}/data-stream`;
      const dataStreamQuery = this.resolvedQuery(dataStreamEndpoint);

      const response = await fetch(dataStreamQuery, {
        body: JSON.stringify({
          config,
          rows: [
            {
              ...data,
              id: uuid(),
              timestamp: Math.round((data.timestamp ?? Date.now()) / 1000),
            },
          ],
        }),
        headers: {
          Authorization: `Bearer ${this.openlayerApiKey}`,
          'Content-Type': 'application/json',
        },
        method: 'POST',
      });

      if (!response.ok) {
        console.error('Error making POST request:', response.status);
        console.error(`Error: ${response.status}`);
      }

      await response.json();
    } catch (error) {
      console.error('Error streaming data to Openlayer:', error);
    }
  };
}

export class OpenAIMonitor {
  private inferencePipeline?: OpenlayerInferencePipeline;

  private openlayerClient: OpenlayerClient;

  private openAIClient: OpenAI;

  private openlayerProjectName: string;

  private openlayerInferencePipelineName: string = 'production';

  private monitoringOn: boolean = false;

  private project?: OpenlayerProject;

  /**
   * Constructs an OpenAIMonitor instance.
   * @param {OpenAIMonitorConstructorProps} props - The configuration properties for the OpenAI and Openlayer clients.
   */
  constructor({
    openAiApiKey,
    openlayerApiKey,
    openlayerProjectName,
    openlayerInferencePipelineName,
    openlayerServerUrl,
  }: OpenAIMonitorConstructorProps) {
    this.openlayerProjectName = openlayerProjectName;
    if (openlayerInferencePipelineName) {
      this.openlayerInferencePipelineName = openlayerInferencePipelineName;
    }

    this.openlayerClient = new OpenlayerClient({
      openlayerApiKey,
      openlayerServerUrl,
    });

    this.openAIClient = new OpenAI({
      apiKey: openAiApiKey,
      dangerouslyAllowBrowser: true,
    });
  }

  private cost = (model: string, inputTokens: number, outputTokens: number) => {
    const pricing: Pricing | undefined = OpenAIPricing[model];
    const inputCost =
      typeof pricing === 'undefined'
        ? undefined
        : (inputTokens / 1000) * pricing.input;
    const outputCost =
      typeof pricing === 'undefined'
        ? undefined
        : (outputTokens / 1000) * pricing.output;
    return typeof pricing === 'undefined'
      ? undefined
      : (inputCost ?? 0) + (outputCost ?? 0);
  };

  private formatChatCompletionInput = (
    messages: ChatCompletionMessageParam[]
  ): ChatCompletionMessageParam[] =>
    messages.map(
      ({ content, role }, i) =>
        ({
          content: role === 'user' ? `{{ message_${i} }}` : content,
          role,
        }) as unknown as ChatCompletionMessageParam
    );

  /**
   * Creates a chat completion using the OpenAI client and streams the result to Openlayer.
   * @param {ChatCompletionCreateParams} body - The parameters for creating a chat completion.
   * @param {RequestOptions} [options] - Optional request options.
   * @returns {Promise<ChatCompletion | Stream<ChatCompletionChunk>>} Promise of a ChatCompletion or a Stream
   * @throws {Error} Throws an error if monitoring is not active or if no output is received from OpenAI.
   */
  public createChatCompletion = async (
    body: ChatCompletionCreateParams,
    options?: RequestOptions,
    additionalLogs?: StreamingData
  ): Promise<ChatCompletion | Stream<ChatCompletionChunk>> => {
    if (!this.monitoringOn) {
      console.warn('Monitoring is not active.');
    } else if (typeof this.inferencePipeline === 'undefined') {
      console.error('No inference pipeline found.');
    }

    // Start a timer to measure latency
    const startTime = Date.now();
    // Accumulate output for streamed responses
    let streamedOutput = '';

    const response = await this.openAIClient.chat.completions.create(
      body,
      options
    );

    try {
      if (this.monitoringOn && typeof this.inferencePipeline !== 'undefined') {
        const prompt = this.formatChatCompletionInput(body.messages);
        const inputVariableNames = prompt
          .filter(({ role }) => role === 'user')
          .map(({ content }) =>
            String(content).replace(/{{\s*|\s*}}/g, '')
          ) as string[];
        const inputVariables = body.messages
          .filter(({ role }) => role === 'user')
          .map(({ content }) => content) as string[];
        const inputVariablesMap = inputVariableNames.reduce(
          (acc, name, i) => ({ ...acc, [name]: inputVariables[i] }),
          {}
        );

        const config = {
          ...this.openlayerClient.defaultConfig,
          inputVariableNames,
          prompt,
        };

        if (body.stream) {
          const streamedResponse = response as Stream<ChatCompletionChunk>;

          for await (const chunk of streamedResponse) {
            // Process each chunk - for example, accumulate input data
            const chunkOutput = chunk.choices[0].delta.content ?? '';
            streamedOutput += chunkOutput;
          }

          const endTime = Date.now();
          const latency = endTime - startTime;

          this.openlayerClient.streamData(
            {
              latency,
              output: streamedOutput,
              timestamp: startTime,
              ...inputVariablesMap,
              ...additionalLogs,
            },
            config,
            this.inferencePipeline.id
          );
        } else {
          const nonStreamedResponse = response as ChatCompletion;
          // Handle regular (non-streamed) response
          const endTime = Date.now();
          const latency = endTime - startTime;
          const output = nonStreamedResponse.choices[0].message.content;
          const tokens = nonStreamedResponse.usage?.total_tokens ?? 0;
          const inputTokens = nonStreamedResponse.usage?.prompt_tokens ?? 0;
          const outputTokens =
            nonStreamedResponse.usage?.completion_tokens ?? 0;
          const cost = this.cost(
            nonStreamedResponse.model,
            inputTokens,
            outputTokens
          );

          if (typeof output === 'string') {
            this.openlayerClient.streamData(
              {
                cost,
                latency,
                model: nonStreamedResponse.model,
                output,
                timestamp: startTime,
                tokens,
                ...inputVariablesMap,
                ...additionalLogs,
              },
              config,
              this.inferencePipeline.id
            );
          } else {
            console.error('No output received from OpenAI.');
          }
        }
      }
    } catch (error) {
      console.error(error);
    }

    return response;
  };

  /**
   * Creates a completion using the OpenAI client and streams the result to Openlayer.
   * @param {CompletionCreateParams} body - The parameters for creating a completion.
   * @param {RequestOptions} [options] - Optional request options.
   * @returns {Promise<Completion | Stream<Completion>>} Promise that resolves to a Completion or a Stream.
   */
  public createCompletion = async (
    body: CompletionCreateParams,
    options?: RequestOptions,
    additionalLogs?: StreamingData
  ): Promise<Completion | Stream<Completion>> => {
    if (!body.prompt) {
      console.error('No prompt provided.');
    }

    if (!this.monitoringOn) {
      console.warn('Monitoring is not active.');
    } else if (typeof this.inferencePipeline === 'undefined') {
      console.error('No inference pipeline found.');
    }

    // Start a timer to measure latency
    const startTime = Date.now();

    // Accumulate output and tokens data for streamed responses
    let streamedModel = body.model;
    let streamedOutput = '';
    let streamedTokens = 0;
    let streamedInputTokens = 0;
    let streamedOutputTokens = 0;

    const response = await this.openAIClient.completions.create(body, options);

    try {
      if (this.monitoringOn && typeof this.inferencePipeline !== 'undefined') {
        const config = {
          ...this.openlayerClient.defaultConfig,
          inputVariableNames: ['input'],
        };

        if (body.stream) {
          const streamedResponse = response as Stream<Completion>;

          for await (const chunk of streamedResponse) {
            // Process each chunk - for example, accumulate input data
            streamedModel = chunk.model;
            streamedOutput += chunk.choices[0].text.trim();
            streamedTokens += chunk.usage?.total_tokens ?? 0;
            streamedInputTokens += chunk.usage?.prompt_tokens ?? 0;
            streamedOutputTokens += chunk.usage?.completion_tokens ?? 0;
          }

          const endTime = Date.now();
          const latency = endTime - startTime;
          const cost = this.cost(
            streamedModel,
            streamedInputTokens,
            streamedOutputTokens
          );

          this.openlayerClient.streamData(
            {
              cost,
              input: body.prompt,
              latency,
              output: streamedOutput,
              timestamp: startTime,
              tokens: streamedTokens,
              ...additionalLogs,
            },
            config,
            this.inferencePipeline.id
          );
        } else {
          const nonStreamedResponse = response as Completion;
          // Handle regular (non-streamed) response
          const endTime = Date.now();
          const latency = endTime - startTime;
          const tokens = nonStreamedResponse.usage?.total_tokens ?? 0;
          const inputTokens = nonStreamedResponse.usage?.prompt_tokens ?? 0;
          const outputTokens =
            nonStreamedResponse.usage?.completion_tokens ?? 0;
          const cost = this.cost(
            nonStreamedResponse.model,
            inputTokens,
            outputTokens
          );

          this.openlayerClient.streamData(
            {
              cost,
              input: body.prompt,
              latency,
              output: nonStreamedResponse.choices[0].text,
              timestamp: startTime,
              tokens,
              ...additionalLogs,
            },
            config,
            this.inferencePipeline.id
          );
        }
      }
    } catch (error) {
      console.error(error);
    }

    return response;
  };

  /**
   * Starts monitoring for the OpenAI Monitor instance. If monitoring is already active, a warning is logged.
   */
  public async startMonitoring() {
    if (this.monitoringOn) {
      console.warn('Monitor is already on.');
      return;
    }

    console.info(
      'Starting monitor: creating or loading an Openlayer project and inference pipeline...'
    );

    try {
      this.monitoringOn = true;
      this.project = await this.openlayerClient.createProject(
        this.openlayerProjectName,
        'llm-base'
      );

      if (typeof this.project !== 'undefined') {
        this.inferencePipeline =
          await this.openlayerClient.createInferencePipeline(
            this.project.id,
            this.openlayerInferencePipelineName
          );
      }

      console.info('Monitor started');
    } catch (error) {
      console.error('An error occurred while starting the monitor:', error);

      this.stopMonitoring();
    }
  }

  /**
   * Stops monitoring for the OpenAI Monitor instance. If monitoring is not active, a warning is logged.
   */
  public stopMonitoring() {
    if (!this.monitoringOn) {
      console.warn('Monitor is not active.');
      return;
    }

    this.monitoringOn = false;
    this.project = undefined;
    this.inferencePipeline = undefined;

    console.info('Monitor stopped.');
  }
}
